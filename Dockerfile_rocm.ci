ARG BASE_DOCKER=rocm/pytorch:rocm7.1_ubuntu24.04_py3.12_pytorch_release_2.9.1
FROM $BASE_DOCKER

ARG PYTORCH_ROCM_ARCH_OVERRIDE="gfx942"

ENV NO_PROXY=localhost,127.0.0.1,pypi.org,files.pythonhosted.org,raw.githubusercontent.com
ENV WORKSPACE_DIR=/workspace
ENV STAGE_DIR=/workspace/installs
ENV PIP_DEFAULT_TIMEOUT=120
ENV PIP_RETRIES=10
RUN mkdir -p $WORKSPACE_DIR
RUN mkdir -p ${STAGE_DIR}
WORKDIR $WORKSPACE_DIR

RUN pip3 install \
scipy \
wheel \
PyGithub \
pytest-xdist \
psutil \
transformers \
einops \
flask-restful \
pytest \
pytest-cov \
pytest_mock \
pytest-csv \
pytest-asyncio \
pytest-random-order \
sentencepiece \
wrapt \
zarr \
wandb \
tensorstore==0.1.45 \
pytest_mock \
pybind11 \
setuptools==69.5.1 \
datasets \
tiktoken \
pynvml

pip install "nltk>=3.8.2"

# Install Causal-Conv1d and its dependencies
WORKDIR ${STAGE_DIR}
ENV CAUSAL_CONV1D_FORCE_BUILD=TRUE
ENV MAMBA_FORCE_BUILD=TRUE
ENV HIP_ARCHITECTURES=${PYTORCH_ROCM_ARCH_OVERRIDE}
RUN git clone https://github.com/Dao-AILab/causal-conv1d causal-conv1d &&\
    cd causal-conv1d &&\
    git show --oneline -s &&\
    pip install --no-build-isolation .

# Install mamba
WORKDIR ${STAGE_DIR}
RUN git clone https://github.com/sudhu2k/mamba mamba &&\
    cd mamba &&\
    git checkout hipcub_warp_threads_deprecation &&\
    git show --oneline -s &&\
    pip install --no-build-isolation --no-deps .

# Build flash-attention
ARG BUILD_FA="1"
ARG FA_BRANCH="main"
ARG FA_REPO="https://github.com/ROCm/flash-attention.git"
WORKDIR $WORKSPACE_DIR
RUN if [ "$BUILD_FA" -eq "1" ]; then \
git clone ${FA_REPO} \
&& cd flash-attention \
&& git checkout ${FA_BRANCH} \
&& MAX_JOBS=128 GPU_ARCHS=${PYTORCH_ROCM_ARCH_OVERRIDE} python3 setup.py install; \
fi

# Clone TE repo and submodules
WORKDIR ${STAGE_DIR}

ARG TE_COMMIT=release_v2.2_rocm
ENV NVTE_FRAMEWORK=pytorch
ENV PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH_OVERRIDE}
ENV NVTE_USE_HIPBLASLT=1

RUN git clone --recursive https://github.com/ROCm/TransformerEngine.git && \
    cd TransformerEngine && \
    git checkout ${TE_COMMIT} && \
    git submodule update --init --recursive && \
    echo "TransformerEngine at commit:" $(git rev-parse HEAD) && \
    pip install --no-build-isolation .

RUN git clone https://github.com/caaatch22/grouped_gemm.git &&\
    cd grouped_gemm &&\
    git checkout rocm &&\
    git submodule update --init --recursive &&\
    pip install --no-build-isolation .

WORKDIR $WORKSPACE_DIR
COPY . Megatron-LM
WORKDIR $WORKSPACE_DIR/Megatron-LM
RUN pip install -e .

ENV PYTHONPATH=/var/lib/jenkins/triton/python

# record configuration for posterity
RUN pip list
