<div align="center">

Megatron-LM & Megatron Core
===========================

<h4>GPU-optimized library for training transformer models at scale</h4>

[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://docs.nvidia.com/Megatron-Core/developer-guide/latest/index.html)
[![version](https://img.shields.io/badge/release-0.12.0-green)](./CHANGELOG.md)
[![license](https://img.shields.io/badge/license-Apache-blue)](./LICENSE)

<div align="left">

> ## ğŸš¨ **DEVELOPMENT BRANCH**
> âš ï¸ **EXPERIMENTAL FEATURES** - This is the **dev branch** with experimental features. 
>
> **â†’ For releases and comprehensive documentation, visit the [main branch](https://github.com/NVIDIA/Megatron-LM)**

## âš¡ Quickstart

```bash
# Clone the dev branch
git clone -b dev https://github.com/NVIDIA/Megatron-LM.git
cd Megatron-LM

# Install from source with dev dependencies (includes transformer_engine)
pip install -e .[mlm,dev]
```

<details>
<summary>Table of Contents</summary>

**Getting Started**
- [âš¡ Quick Start](#-quick-start)
- [ğŸ§  Dev Branch Philosophy](#-dev-branch-philosophy)
- [ğŸ“Š Performance & Benchmarking](#-performance--benchmarking)
- [ğŸ‘¥ Community & Support](#-community--support)

**For Complete Documentation** â†’ [Main Branch](https://github.com/NVIDIA/Megatron-LM) | [Official Docs](https://docs.nvidia.com/Megatron-Core/)

</details>






## Dev Branch Philosophy

### Fast Iteration
- **Streamlined Review**: 1 code owner + 1 dev approver (can delegate review) + CI/CD

### Feature Lifecycle (Coming Soon)
- **6-Month Timeline**: Experimental features must graduate to stable or be deprecated
- **Migration Support**: Assistance provided for feature transitions

### Stability Expectations
- **Experimental Nature**: Features may change or be removed as development progresses
- **Testing**: All features will pass convergence and performance validation before inclusion
- **Support**: Dev branch issues should include `[DEV]` prefix

## Performance & Benchmarking

- ğŸš€ [2025/11] [Optimizing DeepSeek-V3 Training Performance on NVIDIA GB200 NVL72](docs/discussions/deepseek-v3-gb200-optimization/deepseek-v3-gb200-optimization.md).
- âš¡ [2025/11] [A Guide to Reproduce DeepSeek-V3 Pre-training Performance on GB200](docs/discussions/deepseek-v3-gb200-optimization/deepseek-v3-gb200-reproduce-guide.md).

## Community & Support

### Getting Help
- ğŸ“– **[Documentation](https://docs.nvidia.com/Megatron-Core/)** - Official documentation
- ğŸ› **[Issues](https://github.com/NVIDIA/Megatron-LM/issues)** - Bug reports and feature requests

### Contributing
We â¤ï¸ contributions! Ways to contribute:

- ğŸ› **Report bugs** - Help us improve reliability
- ğŸ’¡ **Suggest features** - Shape the future of Megatron Core
- ğŸ“ **Improve docs** - Make Megatron Core more accessible
- ğŸ”§ **Submit PRs** - Contribute code improvements

**â†’ [Contributing Guide](./CONTRIBUTING.md)**

### Citation
```bibtex
@article{megatron-lm,
  title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}
```
