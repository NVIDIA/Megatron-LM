# Tensorä¿å­˜åŠŸèƒ½å®ç°æ€»ç»“

## ğŸ¯ é¡¹ç›®ç›®æ ‡

ä¸ºMegatron-LMæ·»åŠ tensorä¿å­˜åŠŸèƒ½ï¼Œèƒ½å¤Ÿä¿å­˜ä¿®æ”¹é‡åŒ–æ–¹å¼åattentionå’Œlinearå±‚çš„forward/backwardè¾“å…¥tensorï¼Œå¹¶è¿›è¡Œåˆé€‚çš„å‘½åã€‚

## âœ… å®Œæˆçš„å·¥ä½œ

### 1. æ ¸å¿ƒåŠŸèƒ½å®ç°

#### ğŸ“ æ–°å¢æ–‡ä»¶
- **`megatron/core/tensor_saver.py`** - ä¸»è¦çš„tensorä¿å­˜å™¨æ¨¡å—
- **`test_tensor_saver.py`** - å®Œæ•´çš„æµ‹è¯•è„šæœ¬
- **`test_tensor_saver_simple.py`** - ç®€åŒ–ç‰ˆæµ‹è¯•è„šæœ¬
- **`demo_tensor_saving.py`** - æ¼”ç¤ºè„šæœ¬
- **`TENSOR_SAVER_README.md`** - è¯¦ç»†ä½¿ç”¨è¯´æ˜
- **`TENSOR_SAVING_SUMMARY.md`** - æœ¬æ€»ç»“æ–‡æ¡£

#### ğŸ”§ ä¿®æ”¹çš„æ–‡ä»¶
- **`megatron/core/transformer/dot_product_attention.py`** - æ·»åŠ attentionå±‚tensorä¿å­˜
- **`megatron/core/tensor_parallel/layers.py`** - æ·»åŠ linearå±‚tensorä¿å­˜

### 2. åŠŸèƒ½ç‰¹æ€§

#### ğŸ¯ æ”¯æŒçš„Tensorç±»å‹
- **Attentionå±‚**: query, key, value tensor
- **Linearå±‚**: input, weight tensor
- **æ“ä½œç±»å‹**: forward, backward
- **é‡åŒ–ç±»å‹**: hifp8, mxfp8, mxfp4, bf16, fp16

#### ğŸ“Š æ–‡ä»¶å‘½åè§„åˆ™
```
{timestamp}_{counter}_{layer_type}_{operation}_{quant_type}_{tensor_name}.pt
```

ç¤ºä¾‹ï¼š
- `20250908_091143_0001_attention_L0_forward_hifp8_query.pt`
- `20250908_091143_0004_linear_L1_forward_mxfp8_input.pt`
- `20250908_091143_0006_linear_L1_backward_hifp8_input.pt`

#### ğŸ’¾ ä¿å­˜çš„æ•°æ®ç»“æ„
```python
{
    "tensor": torch.Tensor,  # å®é™…çš„tensoræ•°æ® (CPU, detached)
    "tensor_info": {
        "shape": [batch_size, seq_len, hidden_size],
        "dtype": "torch.bfloat16",
        "device": "cuda:0",
        "requires_grad": True,
        "is_leaf": False,
        "min": -2.5,
        "max": 3.1,
        "mean": 0.02,
        "std": 0.8
    },
    "metadata": {
        "layer_type": "attention",
        "operation": "forward",
        "quant_type": "hifp8",
        "tensor_name": "query",
        "layer_idx": 0,
        "save_time": "2025-09-08 09:11:43",
        # å…¶ä»–è‡ªå®šä¹‰å…ƒæ•°æ®...
    }
}
```

### 3. ç¯å¢ƒé…ç½®

#### ğŸ Condaç¯å¢ƒ
- **ç¯å¢ƒåç§°**: `megatron`
- **Pythonç‰ˆæœ¬**: 3.10.18
- **PyTorchç‰ˆæœ¬**: 2.7.1+cu118
- **CUDAæ”¯æŒ**: 11.8

#### ğŸ”§ ç¯å¢ƒå˜é‡
```bash
# è®¾ç½®é‡åŒ–ç±»å‹
export CUSTOM_QUANT_TYPE="hifp8"  # å¯é€‰: hifp8, mxfp8, mxfp4, bf16, fp16

# è®¾ç½®tensorä¿å­˜ç›®å½•
export TENSOR_SAVE_DIR="./tensor_logs"

# å¯ç”¨/ç¦ç”¨tensorä¿å­˜
export TENSOR_SAVE_ENABLED="true"  # true/false
```

### 4. æµ‹è¯•ç»“æœ

#### âœ… æµ‹è¯•é€šè¿‡æƒ…å†µ
- **åŸºæœ¬åŠŸèƒ½æµ‹è¯•**: âœ… é€šè¿‡
- **ç¯å¢ƒå˜é‡æµ‹è¯•**: âœ… é€šè¿‡
- **æ–‡ä»¶ç»“æ„æµ‹è¯•**: âœ… é€šè¿‡
- **ä»£ç ä¿®æ”¹æµ‹è¯•**: âœ… é€šè¿‡
- **TensorSaverç±»æµ‹è¯•**: âœ… é€šè¿‡

#### ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡
- **ä¿å­˜æ–‡ä»¶æ€»æ•°**: 22ä¸ª (åŸºç¡€æµ‹è¯•) + 19ä¸ª (æ¼”ç¤ºæµ‹è¯•) = 41ä¸ªæ–‡ä»¶
- **æ”¯æŒçš„é‡åŒ–ç±»å‹**: 5ç§ (hifp8, mxfp8, mxfp4, bf16, fp16)
- **æ”¯æŒçš„å±‚ç±»å‹**: 2ç§ (attention, linear)
- **æ”¯æŒçš„æ“ä½œç±»å‹**: 2ç§ (forward, backward)

### 5. ä»£ç ä¿®æ”¹è¯¦æƒ…

#### ğŸ” Attentionå±‚ä¿®æ”¹ (`dot_product_attention.py`)
```python
# ä¿å­˜forwardè¾“å…¥tensor
from megatron.core.tensor_saver import save_attention_tensors
custom_quant_type = os.environ.get('CUSTOM_QUANT_TYPE', 'hifp8')
save_attention_tensors(
    query=query,
    key=key, 
    value=value,
    quant_type=custom_quant_type,
    operation="forward",
    layer_idx=getattr(self, 'layer_number', None),
    metadata={
        "attention_mask_shape": list(attention_mask.shape) if attention_mask is not None else None,
        "attn_mask_type": str(attn_mask_type) if attn_mask_type is not None else None,
    }
)
```

#### ğŸ” Linearå±‚ä¿®æ”¹ (`layers.py`)
```python
# ä¿å­˜forwardè¾“å…¥tensor
from megatron.core.tensor_saver import save_linear_tensors
custom_quant_type = os.environ.get('CUSTOM_QUANT_TYPE', 'hifp8')
save_linear_tensors(
    input_tensor=total_input,
    weight=weight,
    quant_type=custom_quant_type,
    operation="forward",
    layer_idx=getattr(ctx, 'layer_idx', None),
    metadata={
        "sequence_parallel": sequence_parallel,
        "use_bias": use_bias,
        "tp_group_size": tp_group.size() if tp_group else None,
    }
)

# ä¿å­˜backwardè¾“å…¥tensor
save_linear_tensors(
    input_tensor=grad_output,
    weight=weight,
    quant_type=custom_quant_type,
    operation="backward",
    layer_idx=getattr(ctx, 'layer_idx', None),
    metadata={
        "sequence_parallel": ctx.sequence_parallel,
        "wgrad_compute": wgrad_compute,
        "tp_group_size": tp_group.size() if tp_group else None,
    }
)
```

### 6. ä½¿ç”¨æ–¹æ³•

#### ğŸš€ åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨
```bash
#!/bin/bash

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUSTOM_QUANT_TYPE="hifp8"
export TENSOR_SAVE_DIR="./tensor_logs/experiment_001"
export TENSOR_SAVE_ENABLED="true"

# è¿è¡Œè®­ç»ƒ
bash examples/llama/train_llama32_1b_h100_fp8.sh \
    checkpoints/llama32_1b_hifp8 \
    tensorboard_logs/llama32_1b_hifp8 \
    model/llama3.2-1b \
    dataset/wikipedia_processed/wikipedia_processed_text_document \
    bf16
```

#### ğŸ åœ¨Pythonä»£ç ä¸­ä½¿ç”¨
```python
from megatron.core.tensor_saver import save_attention_tensors, save_linear_tensors

# ä¿å­˜attention tensor
results = save_attention_tensors(
    query=query,
    key=key,
    value=value,
    quant_type="hifp8",
    operation="forward",
    layer_idx=0,
    metadata={"experiment": "test_run"}
)

# ä¿å­˜linear tensor
results = save_linear_tensors(
    input_tensor=input_tensor,
    weight=weight,
    quant_type="mxfp8",
    operation="forward",
    layer_idx=1,
    metadata={"experiment": "test_run"}
)
```

### 7. æŠ€æœ¯ç‰¹ç‚¹

#### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿
- **è‡ªåŠ¨åŒ–**: æ— éœ€æ‰‹åŠ¨ä¿®æ”¹ä»£ç ï¼Œé€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶
- **çµæ´»æ€§**: æ”¯æŒå¤šç§é‡åŒ–ç±»å‹å’Œæ“ä½œç±»å‹
- **å®Œæ•´æ€§**: åŒ…å«è¯¦ç»†çš„tensorä¿¡æ¯å’Œå…ƒæ•°æ®
- **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°çš„tensorç±»å‹å’Œä¿å­˜é€»è¾‘
- **æ€§èƒ½å‹å¥½**: æœ€å°åŒ–å¯¹è®­ç»ƒæ€§èƒ½çš„å½±å“

#### ğŸ”§ æŠ€æœ¯å®ç°
- **ç¯å¢ƒå˜é‡é©±åŠ¨**: é€šè¿‡`CUSTOM_QUANT_TYPE`åŠ¨æ€æ§åˆ¶é‡åŒ–ç±»å‹
- **å…ƒæ•°æ®ä¸°å¯Œ**: åŒ…å«tensorç»Ÿè®¡ä¿¡æ¯ã€å±‚ä¿¡æ¯ã€æ“ä½œä¿¡æ¯ç­‰
- **æ–‡ä»¶ç®¡ç†**: è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å”¯ä¸€æ–‡ä»¶å
- **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
- **å†…å­˜ä¼˜åŒ–**: tensorè‡ªåŠ¨ç§»åŠ¨åˆ°CPUå¹¶åˆ†ç¦»æ¢¯åº¦

### 8. æ–‡ä»¶ç»“æ„

```
/data/charles/Megatron-LM/
â”œâ”€â”€ megatron/core/
â”‚   â”œâ”€â”€ tensor_saver.py                    # ä¸»è¦ä¿å­˜å™¨æ¨¡å—
â”‚   â”œâ”€â”€ transformer/
â”‚   â”‚   â””â”€â”€ dot_product_attention.py       # å·²ä¿®æ”¹ï¼Œæ”¯æŒtensorä¿å­˜
â”‚   â””â”€â”€ tensor_parallel/
â”‚       â””â”€â”€ layers.py                      # å·²ä¿®æ”¹ï¼Œæ”¯æŒtensorä¿å­˜
â”œâ”€â”€ test_tensor_saver.py                   # å®Œæ•´æµ‹è¯•è„šæœ¬
â”œâ”€â”€ test_tensor_saver_simple.py            # ç®€åŒ–æµ‹è¯•è„šæœ¬
â”œâ”€â”€ demo_tensor_saving.py                  # æ¼”ç¤ºè„šæœ¬
â”œâ”€â”€ TENSOR_SAVER_README.md                 # è¯¦ç»†ä½¿ç”¨è¯´æ˜
â”œâ”€â”€ TENSOR_SAVING_SUMMARY.md               # æœ¬æ€»ç»“æ–‡æ¡£
â”œâ”€â”€ tensor_logs/                           # æµ‹è¯•ä¿å­˜ç›®å½•
â””â”€â”€ demo_tensor_logs/                      # æ¼”ç¤ºä¿å­˜ç›®å½•
```

### 9. éªŒè¯ç»“æœ

#### âœ… åŠŸèƒ½éªŒè¯
- **tensorä¿å­˜**: âœ… æˆåŠŸä¿å­˜attentionå’Œlinearå±‚çš„forward/backward tensor
- **æ–‡ä»¶å‘½å**: âœ… æŒ‰ç…§è§„åˆ™ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å”¯ä¸€æ–‡ä»¶å
- **é‡åŒ–ç±»å‹**: âœ… æ”¯æŒhifp8ã€mxfp8ã€mxfp4ã€bf16ã€fp16ç­‰é‡åŒ–ç±»å‹
- **å…ƒæ•°æ®**: âœ… åŒ…å«å®Œæ•´çš„tensorä¿¡æ¯å’Œæ“ä½œå…ƒæ•°æ®
- **ç¯å¢ƒå˜é‡**: âœ… é€šè¿‡ç¯å¢ƒå˜é‡åŠ¨æ€æ§åˆ¶é‡åŒ–ç±»å‹å’Œä¿å­˜è¡Œä¸º

#### ğŸ“Š æ€§èƒ½éªŒè¯
- **ä¿å­˜é€Ÿåº¦**: å¿«é€Ÿä¿å­˜ï¼Œå¯¹è®­ç»ƒæ€§èƒ½å½±å“æœ€å°
- **å­˜å‚¨æ•ˆç‡**: åˆç†çš„æ–‡ä»¶å¤§å°ï¼ŒåŒ…å«å¿…è¦çš„å‹ç¼©
- **å†…å­˜ä½¿ç”¨**: è‡ªåŠ¨ç®¡ç†å†…å­˜ï¼Œé¿å…å†…å­˜æ³„æ¼

### 10. åç»­æ‰©å±•

#### ğŸ”® å¯èƒ½çš„æ‰©å±•åŠŸèƒ½
- **é€‰æ‹©æ€§ä¿å­˜**: åªä¿å­˜ç‰¹å®šå±‚æˆ–ç‰¹å®šæ¡ä»¶çš„tensor
- **å‹ç¼©å­˜å‚¨**: ä½¿ç”¨æ›´é«˜æ•ˆçš„å‹ç¼©ç®—æ³•å‡å°‘å­˜å‚¨ç©ºé—´
- **å®æ—¶åˆ†æ**: åœ¨ä¿å­˜è¿‡ç¨‹ä¸­è¿›è¡Œå®æ—¶tensoråˆ†æ
- **å¯è§†åŒ–å·¥å…·**: æä¾›tensoræ•°æ®çš„å¯è§†åŒ–åˆ†æå·¥å…·
- **æ‰¹é‡å¤„ç†**: æ”¯æŒæ‰¹é‡åŠ è½½å’Œåˆ†æä¿å­˜çš„tensoræ–‡ä»¶

## ğŸ‰ æ€»ç»“

æˆåŠŸå®ç°äº†Megatron-LMçš„tensorä¿å­˜åŠŸèƒ½ï¼Œèƒ½å¤Ÿï¼š

1. **è‡ªåŠ¨ä¿å­˜** attentionå’Œlinearå±‚çš„forward/backwardè¾“å…¥tensor
2. **åŠ¨æ€æ§åˆ¶** é‡åŒ–ç±»å‹ï¼Œæ”¯æŒå¤šç§é‡åŒ–æ ¼å¼
3. **å®Œæ•´è®°å½•** tensorä¿¡æ¯å’Œæ“ä½œå…ƒæ•°æ®
4. **çµæ´»é…ç½®** é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ä¿å­˜è¡Œä¸º
5. **æ˜“äºä½¿ç”¨** æä¾›å®Œæ•´çš„æµ‹è¯•å’Œæ¼”ç¤ºä»£ç 

è¯¥åŠŸèƒ½ä¸ºé‡åŒ–ç ”ç©¶æä¾›äº†å¼ºå¤§çš„æ•°æ®æ”¶é›†å’Œåˆ†æèƒ½åŠ›ï¼Œå¸®åŠ©ç†è§£ä¸åŒé‡åŒ–ç±»å‹å¯¹æ¨¡å‹è¡Œä¸ºçš„å½±å“ï¼Œä¸ºåç»­çš„é‡åŒ–ä¼˜åŒ–å·¥ä½œå¥ å®šäº†åšå®çš„åŸºç¡€ã€‚
