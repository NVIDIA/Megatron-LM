torchrun --nproc-per-node 1 \
    -m examples.inference.gpt.gpt_dynamic_inference \
    --load /lustre/fsw/portfolios/coreai/users/shanmugamr/Megatron-LM/deepseek_mtp_dummy_ckpt/checkpoints \
    --bf16 \
    --model-provider gpt \
    --tensor-model-parallel-size 1 \
    --micro-batch-size 16 \
    --num-tokens-to-generate 20 \
    --inference-dynamic-batching-buffer-size-gb 5 \
    --prompt-file /lustre/fsw/portfolios/llmservice/users/ksanthanam/megatron-lm/debug_prompts.jsonl \
    --use-checkpoint-args \
    --enable-cuda-graph \
    --incoming-requests-per-sec 16 \
    --dist-ckpt-strictness log_unexpected \
    --decode-only-cuda-graphs \
    --tokenizer-type HuggingFaceTokenizer \
    --tokenizer-model deepseek-ai/deepseek-coder-6.7b-base \
    --no-use-tokenizer-model-from-checkpoint-args \
    --output-path /lustre/fsw/portfolios/coreai/users/shanmugamr/Megatron-LM/output.json \
    --return-log-probs