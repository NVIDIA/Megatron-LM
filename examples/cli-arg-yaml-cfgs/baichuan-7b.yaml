bf16: True
num_layers: 32
hidden_size: 4096
ffn_hidden_size: 11008
num_attention_heads: 32
num_query_groups: 32
max_position_embeddings: 4096
norm_epsilon: 1.0e-05
rotary_base: 10000
position_embedding_type: 'rope'
normalization: 'RMSNorm'
add_bias_linear: False
masked_softmax_fusion: False
swiglu: True
attention_dropout: 0.0
hidden_dropout: 0.0
group_query_attention: True
untie_embeddings_and_output_weights: True
