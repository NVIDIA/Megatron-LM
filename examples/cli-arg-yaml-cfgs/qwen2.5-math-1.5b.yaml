bf16: True
num_layers: 28
hidden_size: 1536
ffn_hidden_size: 8960
num_attention_heads: 12
num_query_groups: 2
max_position_embeddings: 4096
norm_epsilon: 1.0e-06
rotary_base: 10000
position_embedding_type: 'rope'
normalization: 'RMSNorm'
masked_softmax_fusion: False
swiglu: True
attention_dropout: 0.0
hidden_dropout: 0.0
group_query_attention: True
add_bias_linear: False
add_qkv_bias: True
padded_vocab_size: 151936
