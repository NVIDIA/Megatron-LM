# Data Processing for LLM Pretraining

Here is a tree of this folder and a short description for each of the file. 

```
.
├── README.md # README for data processing.
├── az_templates # Azure templates to run merge_shards.py and tokenize_shards.py.
│   ├── template_merge_shard.yaml 
│   ├── template_nemo_tokenize.yaml
│   └── template_tokenize.yaml
├── count_token_and_rename_bin_idx.py # Given a .bin and .idx file, it calculates the number of documents, sentences, and tokens.
├── data_ratio_from_file.py # Calculates iterator selection probability.
├── data_signature # Contains all the data signatures for the processed files. It depends on the tokenizer and data shards themselves. These are considered data assets and are kept in the GitHub repository for archival purposes. 
│   ├── allam_data_2-1_splits-llama2-VE-indexed_data.json
│   ├── allam_data_2-1_splits-llama2-indexed_data.json
│   └── the_pile_tokenizer_v5_improved.json
├── docs # Documentation for all the important scripts.
│   ├── count_token_and_rename_bin_idx.md
│   ├── data_ratio_from_file.md
│   ├── merge_shards.md
│   ├── multiprocess_runner.md
│   ├── remote_list.md
│   └── tokenize_shards.md
├── merge_shards.py # Given a folder that contains many smaller shards, it combines that data into specified sizes.
├── merge_shards_runner # Runner scripts for merge shard processing. This is not experiment-dependent and is done once on the downloaded data based on shard size requirements. 
│   ├── merge_shards_data_ar_translated.sh
│   ├── merge_shards_dolma.sh
│   ├── merge_shards_pile.sh
│   ├── merge_shards_slim_pajama.sh
│   └── slim_pajama_process.sh
├── multiprocess_runner.py # A multiprocessor runner for local tokenization of data. 
├── remote_list.py # Lists remote files and saves their data signatures. 
├── remote_scripts # Scripts that usually run on remote machines (i.e., Azure). 
│   ├── remote_az_batch_tokenize.sh
│   └── remote_merge_shard.sh
├── sample_json_data_from_meta.py # Extracts data from shards based on meta-information and saves it to disk. 
├── tokenize_shards.py # Tokenizes the shards.
└── word_count.py # Counts the number of words in a JSONL file.
```

Documentation for all the scripts can be located in the `examples/data-processing/docs` folder. These documentations have been generated by ChatGPT and have undergone review.

For LLM data processing, follow these steps once the data is in the Azure container:

## Merge the Shards
   - To train the data, merge the shards based on domains. You can find all the merge scripts in the `examples/data-processing/merge_shards_runner/` folder. It's important to note that this step is not dependent on the model, which is why these scripts are located outside of the experiments folder.
   - Documentation of the merge shards are [here](./docs/merge_shards.md)

   **TODO (Additional PR Required):**
   - The merge script cannot split a shard into smaller pieces if the shard size is too large.
   - If the file sizes are too small but the requested shard size is too large, the script won't function as expected. Refer to the script's `Note` section for a better understanding.

## Tokenize the Shards
   - Tokenize the data and store it in a specific memory-mapped format. This is done in both NeMo and Megatron-LM. This section is an integral part of the experiment and should be placed within the root directory of the experiment folder (i.e., `examples/data-processing/pretrain-llama`).
   - Documentation of the tokenizations are [here](./docs/tokenize_shards.md)

   **TODO (Additional PR Required):**
   - NeMo's tokenization consumes excessive memory, rendering it unsuitable for 128GB CPU nodes. Investigate and resolve memory leaks.

## Calculate the Iterator Probability 
    
This step involves experiment-specific arguments and should be placed inside each of the experiment folders (e.g., `examples/data-processing/pretrain-llama/training/llama_en_reasoning`). This represents the lowest level of the experiment folder hierarchy. For each of the experiments, we use the `examples/data-processing/data_ratio_from_file.py` script to obtain the iterator selection probability. Please refer to the script documentation for a detailed understanding.

    
    usage: data_ratio_from_file.py [-h] [--source-prefix-paths SOURCE_PREFIX_PATHS] [--prefix-paths-from-json PREFIX_PATHS_FROM_JSON]
                               --domain-ratio-from-json DOMAIN_RATIO_FROM_JSON --lang-select-prob-json LANG_SELECT_PROB_JSON
                               --exclude-iterator-json EXCLUDE_ITERATOR_JSON --total-token TOTAL_TOKEN [--verbose]
                               [--export-for-script EXPORT_FOR_SCRIPT] [--prefix-for-file-path PREFIX_FOR_FILE_PATH]

    options:
    -h, --help            display this help message and exit
    --source-prefix-paths SOURCE_PREFIX_PATHS
                            Glob path to the folder where all the bin and idx files are located.
    --prefix-paths-from-json PREFIX_PATHS_FROM_JSON
                            File names listed in a JSON file.
    --domain-ratio-from-json DOMAIN_RATIO_FROM_JSON
                            Domain multiplier from a JSON file.
    --lang-select-prob-json LANG_SELECT_PROB_JSON
                            Path to a JSON file that indicates the language selection probability.
    --exclude-iterator-json EXCLUDE_ITERATOR_JSON
                            Path to a JSON file that lists the restricted iterator names.
    --total-token TOTAL_TOKEN
                            Total tokens to be sampled.
    --verbose             Print additional information.
    --export-for-script EXPORT_FOR_SCRIPT
                            Export output to this file for running it in Megatron format arguments.
    --prefix-for-file-path PREFIX_FOR_FILE_PATH
                            Add an additional prefix to the file path.


Documentation of the iterator selection probability script is [here](./docs/data_ratio_from_file.md)