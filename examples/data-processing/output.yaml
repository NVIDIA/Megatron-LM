$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
code: ../../../../
command: 'bash examples/pretrain-llama/data-processing/merge_shard/remote_merge_shard.sh
  \

  "https://allamllmuksstandard.blob.core.windows.net/vocab-expanded-training-data/dolma/data/c4/"
  \

  "https://allamllmuksstandard.blob.core.windows.net/vocab-expanded-training-data/dolma/merged_shards/"
  \

  "sp=racwdli&st=2023-11-28T16:58:48Z&se=2024-08-31T00:58:48Z&spr=https&sv=2022-11-02&sr=c&sig=DzkkHockgmRh98GuuVd6%2Br0DBcBTdFDnDFp%2FADy9xww%3D"
  \

  "en_dolma_c4_000.jsonl" \

  "c4-0000.json" \

  "c4-0001.json" \

  "c4-0002.json" \

  "c4-0003.json"'
compute: azureml:LARGE-CPU
display_name: merge_shard
distribution:
  process_count_per_instance: 1
  type: pytorch
environment: azureml:Megatron-LLaMA:7
environment_variables:
  CUDA_DEVICE_MAX_CONNECTIONS: '1'
  HYDRA_FULL_ERROR: '1'
  NCCL_DEBUG: WARN
  NCCL_IB_PCI_RELAXED_ORDERING: '2'
  NCCL_IB_TIMEOUT: '22'
  UCX_IB_PCI_RELAXED_ORDERING: auto
experiment_name: tokenize
resources:
  instance_count: 1
  shm_size: 100g
services:
  my_jupyterlab:
    nodes: all
    type: jupyter_lab
