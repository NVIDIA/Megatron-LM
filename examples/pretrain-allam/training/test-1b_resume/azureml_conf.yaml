# Training job submission via AML CLI v2

$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json

command: >-
  bash examples/pretrain-allam/training/test-1b_resume/run.sh \
   ${{inputs.tokenizer_model_path}} \
   ${{inputs.bin_idx_data_dir}} \
   ${{outputs.data_cache}} \
   ${{outputs.checkpoint_dir}} \
   ${{outputs.checkpoint_dir}} 

display_name: test-1b_resume

experiment_name: 1b_mlm
code: ../../../../
environment: azureml:Allam-MegatronLM:2
environment_variables:
  HYDRA_FULL_ERROR: '1'
  UCX_IB_PCI_RELAXED_ORDERING: 'auto'
  NCCL_IB_PCI_RELAXED_ORDERING: '2'
  NCCL_IB_TIMEOUT: '22'
  NCCL_DEBUG: WARN
  CUDA_DEVICE_MAX_CONNECTIONS: '1'
inputs:
  tokenizer_model_path:
    type: uri_file
    mode: ro_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/LLM-Test/workspaces/Provisioning-Test/datastores/allam_tokenizer/paths/tokenizer_v5_improved/ar_en.model
  bin_idx_data_dir:
    type: uri_folder
    mode: ro_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/LLM-Test/workspaces/Provisioning-Test/datastores/allam_ix_data_allamllmuksstandard/paths/the_pile_bin_idx_sp_with_data_signature/the_pile_bin_idx_sp/
outputs:
  data_cache:
    type: uri_folder
    mode: rw_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/LLM-Test/workspaces/Provisioning-Test/datastores/allam_index_mapped_data_allamllmuksstandard/paths/test-1b_resume/
  checkpoint_dir:
    type: uri_folder
    mode: rw_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/LLM-Test/workspaces/Provisioning-Test/datastores/allam_checkpoints_allamllmuksstandard/paths/test-1b_resume/


services:
  my_jupyterlab:
    type: jupyter_lab
  my_tensorboard:
    type: tensor_board
    log_dir: ${{outputs.checkpoint_dir}}

compute: azureml:NCAI-A100-LLMv3

distribution:
  type: pytorch
  process_count_per_instance: 8

resources:
  instance_count: 2
  shm_size: 1250g