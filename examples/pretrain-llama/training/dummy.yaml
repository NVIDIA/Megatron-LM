# Training job submission via AML CLI v2

$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json

command: >-
  sleep infinity
  

display_name: MegatronLM-Wikipedia_Ar_En_Experiment_tp2_pp2_seq2048_bs1976

experiment_name: MegatronLM-Vocab-Expansion
code: ../../../
environment: azureml:Megatron-LLaMA:4
environment_variables:
  HYDRA_FULL_ERROR: '1'
  UCX_IB_PCI_RELAXED_ORDERING: 'auto'
  NCCL_IB_PCI_RELAXED_ORDERING: '2'
  NCCL_IB_TIMEOUT: '22'
  NCCL_DEBUG: WARN
  CUDA_DEVICE_MAX_CONNECTIONS: '1'
inputs:
  load_initial_model:
    type: uri_folder
    mode: ro_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/LLM-Test/workspaces/Provisioning-Test/datastores/srashed/paths/vocab_expansion_assets/megatronlm_models/Llama-2-7b-hf-ExpTok-32K_10M_tp2_pp2_seq4096_gb48/
  tokenizer:
    type: uri_file
    mode: ro_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/llm-test/workspaces/Provisioning-Test/datastores/llama_pretraining/paths/allam_data_2-1_splits-llama2-VE-indexed_data/tokenizer/tokenizer.model
  data:
    type: uri_folder
    mode: ro_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/llm-test/workspaces/Provisioning-Test/datastores/llama_pretraining/paths/allam_data_2-1_splits-llama2-VE-indexed_data/bin_idx/
outputs:
  cache:
    type: uri_folder
    mode: rw_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/llm-test/workspaces/Provisioning-Test/datastores/sbmaruf/paths/vocab_expansion_assets/experiments/Llama-2-7b-hf-ExpTok-32K_10M_tp4_pp2_seq2048_en_all_gbs1920/cache
  model_dir:
    type: uri_folder
    mode: rw_mount
    path: azureml://subscriptions/c7209a17-0d9f-41df-8e45-e0172343698d/resourcegroups/llm-test/workspaces/Provisioning-Test/datastores/sbmaruf/paths/vocab_expansion_assets/experiments/Llama-2-7b-hf-ExpTok-32K_10M_tp4_pp2_seq2048_en_all_gbs1920/


services:
  my_jupyterlab:
    type: jupyter_lab
    nodes: all

compute: azureml:A100-SSH

distribution:
  type: pytorch
  process_count_per_instance: 8

resources:
  instance_count: 1
  shm_size: 800g