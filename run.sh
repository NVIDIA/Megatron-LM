export MASTER_ADDR="localhost"
export MASTER_PORT="12345"
python pretrain_gpt.py --virtual-tensor-model-parallel-size 2 --virtual-pipeline-model-parallel-size 2 --seq-length 2048 --max-position-embeddings 2048 --tokenizer-type "Llama2Tokenizer" --tokenizer-model "../DUMPED/tokenizer.model" --load "../DUMPED/Llama-2-7b-hf-ExpTok-32K_10M_tp2_pp2_seq4096_gb48/" --exit-on-missing-checkpoint --use-checkpoint-args --no-initialization --no-load-optim --no-load-rng --bf16 --untie-embeddings-and-output-weights --use-rotary-position-embeddings --normalization "RMSNorm" --no-position-embedding --no-masked-softmax-fusion --no-query-key-layer-scaling --micro-batch-size 1 --global-batch-size 1 --train-iters 3000 --lr 0.0003  --lr-decay-style "cosine" --min-lr 0.00003  --lr-warmup-iters 20  --weight-decay 0.1  --clip-grad 1.0  --save "../DUMPED/save" --data-path 1.0 "../DUMPED/ar_books_split_00_text_document" --data-cache-path "../DUMPED/cache" --split "9998,1,1" --log-interval 10 --log-validation-ppl-to-tensorboard  --save-interval 200  --eval-interval 200  --eval-iters 100 --tensorboard-dir "../DUMPED/save" --tensorboard-log-interval 100 --no-async-tensor-model-parallel-allreduce --use-flash-attn