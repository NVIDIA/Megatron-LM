# Megatron-LM é‡åŒ–è®­ç»ƒè„šæœ¬ç³»ç»Ÿ

## ğŸ“‹ æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„Megatron-LMé‡åŒ–è®­ç»ƒè„šæœ¬ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æ¨¡å‹ã€æ•°æ®é›†å’Œé‡åŒ–æ–¹å¼çš„ç»„åˆè®­ç»ƒã€‚ç³»ç»Ÿæä¾›äº†çµæ´»çš„é…ç½®ç®¡ç†å’Œç²¾ç¡®çš„é‡åŒ–æ§åˆ¶ã€‚

## ğŸ¯ æ”¯æŒé…ç½®

### æ¨¡å‹æ”¯æŒ (3ä¸ª)
- **llama31-8b**: LLaMA 3.1 8Bæ¨¡å‹
- **llama32-1b**: LLaMA 3.2 1Bæ¨¡å‹  
- **deepseek2_lite**: DeepSeek2 Liteæ¨¡å‹

### æ•°æ®é›†æ”¯æŒ (2ä¸ª)
- **wikipedia**: Wikipediaæ•°æ®é›†
- **dolma**: Dolmaæ•°æ®é›†

### é‡åŒ–æ–¹å¼æ”¯æŒ (10ç§)

#### 1. æ— é‡åŒ–
- **bf16**: BF16ç²¾åº¦ï¼Œæ— é‡åŒ–

#### 2. FAé‡åŒ– (Flash Attentioné‡åŒ–)
- **FA_mxfp8**: åªå¯¹Flash Attentionçš„QKè¿›è¡ŒMXFP8é‡åŒ–
- **FA_mxfp4**: åªå¯¹Flash Attentionçš„QKè¿›è¡ŒMXFP4é‡åŒ–
- **FA_hifp8**: åªå¯¹Flash Attentionçš„QKè¿›è¡ŒHIFP8é‡åŒ–

#### 3. Linearé‡åŒ– (çº¿æ€§å±‚é‡åŒ–)
- **linear_mxfp8**: åªå¯¹çº¿æ€§å±‚è¿›è¡ŒMXFP8é‡åŒ–
- **linear_mxfp4**: åªå¯¹çº¿æ€§å±‚è¿›è¡ŒMXFP4é‡åŒ–
- **linear_hifp8**: åªå¯¹çº¿æ€§å±‚è¿›è¡ŒHIFP8é‡åŒ–

#### 4. FA+Linearé‡åŒ– (ç»„åˆé‡åŒ–)
- **FA_linear_mxfp8**: FA QK + Linearå±‚éƒ½è¿›è¡ŒMXFP8é‡åŒ–
- **FA_linear_mxfp4**: FA QK + Linearå±‚éƒ½è¿›è¡ŒMXFP4é‡åŒ–
- **FA_linear_hifp8**: FA QK + Linearå±‚éƒ½è¿›è¡ŒHIFP8é‡åŒ–

## ğŸ› ï¸ æ ¸å¿ƒå·¥å…·

### 1. è®­ç»ƒé…ç½®è„šæœ¬
- **`train_config.py`**: ä¸»è¦è®­ç»ƒé…ç½®è„šæœ¬ï¼Œæ”¯æŒæ‰€æœ‰æ¨¡å‹å’Œé‡åŒ–ç»„åˆ
- **`flash_attention_config.py`**: Flash Attentionä¸“ç”¨é…ç½®è„šæœ¬

### 2. é‡åŒ–æ§åˆ¶å·¥å…·
- **`quant_type_modifier.py`**: é‡åŒ–ç±»å‹ä¿®æ”¹å·¥å…·ï¼Œç”¨äºä¿®æ”¹æºç ä¸­çš„é‡åŒ–è®¾ç½®

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

#### ä½¿ç”¨è®­ç»ƒé…ç½®è„šæœ¬
```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é…ç½®
python3 train_config.py --list

# è¿è¡Œè®­ç»ƒ
python3 train_config.py --model llama31-8b --dataset wikipedia --quantization bf16
python3 train_config.py --model deepseek2_lite --dataset dolma --quantization FA_linear_mxfp8

# å¿«é€Ÿæµ‹è¯•
python3 train_config.py --model llama32-1b --dataset wikipedia --quantization linear_mxfp4 --training-config fast

# é¢„è§ˆå‘½ä»¤ï¼ˆä¸å®é™…æ‰§è¡Œï¼‰
python3 train_config.py --model llama31-8b --dataset wikipedia --quantization mxfp8 --dry-run
```

#### ä½¿ç”¨Flash Attentioné…ç½®è„šæœ¬
```bash
# æŸ¥çœ‹Flash Attentioné‡åŒ–é€‰é¡¹
python3 flash_attention_config.py --list

# è¿è¡ŒFlash Attentioné‡åŒ–è®­ç»ƒ
python3 flash_attention_config.py --model llama31-8b --dataset wikipedia --fa-quantization qk_mxfp8
```

#### ä½¿ç”¨å•ç‹¬çš„è®­ç»ƒè„šæœ¬
```bash
# FAé‡åŒ– (åªé‡åŒ–Flash Attentionçš„QK)
./llama31-8b/pretrain_llama31-8b_wikipedia_FA_mxfp8.sh
./llama32-1b/pretrain_llama32-1b_dolma_FA_mxfp4.sh

# Linearé‡åŒ– (åªé‡åŒ–çº¿æ€§å±‚)
./llama31-8b/pretrain_llama31-8b_wikipedia_linear_mxfp8.sh
./deepseek2_lite/pretrain_deepseek2_lite_wikipedia_linear_hifp8.sh

# FA+Linearé‡åŒ– (ç»„åˆé‡åŒ–)
./llama31-8b/pretrain_llama31-8b_wikipedia_FA_linear_mxfp8.sh
./deepseek2_lite/pretrain_deepseek2_lite_dolma_FA_linear_hifp8.sh
```

## ğŸ”§ é‡åŒ–æ§åˆ¶æœºåˆ¶

### é‡è¦å‘ç°

**åœ¨å½“å‰çš„Megatron-LMå®ç°ä¸­ï¼Œé‡åŒ–ç±»å‹æ˜¯é€šè¿‡ç¡¬ç¼–ç çš„ `custom_quant_type` å˜é‡æ§åˆ¶çš„ï¼Œè€Œä¸æ˜¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ï¼**

### é‡åŒ–æ§åˆ¶ä½ç½®

#### 1. Linearå±‚é‡åŒ–æ§åˆ¶
**æ–‡ä»¶**: `megatron/core/tensor_parallel/layers.py`  
**ä½ç½®**: ç¬¬783è¡Œ

```python
# å½“å‰ç¡¬ç¼–ç å€¼
custom_quant_type = 'hifp8'
```

#### 2. Attention QKè®¡ç®—é‡åŒ–æ§åˆ¶
**æ–‡ä»¶**: `megatron/core/transformer/dot_product_attention.py`  
**ä½ç½®**: ç¬¬166è¡Œ

```python
# å½“å‰ç¡¬ç¼–ç å€¼
custom_quant_type = 'hifp8'
```

#### 3. Attention PVè®¡ç®—é‡åŒ–æ§åˆ¶
**æ–‡ä»¶**: `megatron/core/transformer/dot_product_attention.py`  
**ä½ç½®**: ç¬¬238è¡Œ

```python
# å½“å‰ç¡¬ç¼–ç å€¼
custom_quant_type = 'hifp8'
```

### ä½¿ç”¨é‡åŒ–ä¿®æ”¹å·¥å…·

#### æ£€æŸ¥å½“å‰çŠ¶æ€
```bash
python3 quant_type_modifier.py --check
```

#### ä¿®æ”¹é‡åŒ–ç±»å‹
```bash
# ä¿®æ”¹Linearå±‚ä¸ºMXFP8
python3 quant_type_modifier.py --linear-quant mxfp8

# ä¿®æ”¹QKè®¡ç®—ä¸ºMXFP8
python3 quant_type_modifier.py --qk-quant mxfp8

# ä¿®æ”¹PVè®¡ç®—ä¸ºMXFP8
python3 quant_type_modifier.py --pv-quant mxfp8

# åŒæ—¶ä¿®æ”¹å¤šä¸ªé‡åŒ–ç±»å‹
python3 quant_type_modifier.py --linear-quant mxfp8 --qk-quant mxfp8 --pv-quant hifp8
```

#### æ¢å¤åŸå§‹è®¾ç½®
```bash
python3 quant_type_modifier.py --restore
```

### æ”¯æŒçš„é‡åŒ–ç±»å‹

- `'hifp8'`: HIFP8é‡åŒ– (å½“å‰é»˜è®¤)
- `'mxfp8'`: MXFP8é‡åŒ–
- `'mxfp4'`: MXFP4é‡åŒ–
- `'none'` æˆ–å…¶ä»–å€¼: æ— é‡åŒ–ï¼Œä½¿ç”¨æ ‡å‡†PyTorchæ“ä½œ

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é‡åŒ–ç±»å‹ | å†…å­˜èŠ‚çœ | è®¡ç®—åŠ é€Ÿ | ç²¾åº¦ä¿æŒ | æ¨èåœºæ™¯ |
|----------|----------|----------|----------|----------|
| bf16 | 0% | åŸºå‡† | æœ€é«˜ | ç²¾åº¦ä¼˜å…ˆ |
| FA_mxfp8 | ~15% | +10% | é«˜ | æ³¨æ„åŠ›ä¼˜åŒ– |
| FA_mxfp4 | ~25% | +20% | ä¸­ç­‰ | æ³¨æ„åŠ›å¤§å¹…ä¼˜åŒ– |
| linear_mxfp8 | ~20% | +15% | é«˜ | çº¿æ€§å±‚ä¼˜åŒ– |
| linear_mxfp4 | ~30% | +25% | ä¸­ç­‰ | çº¿æ€§å±‚å¤§å¹…ä¼˜åŒ– |
| FA_linear_mxfp8 | ~35% | +25% | ä¸­ç­‰ | å…¨é¢ä¼˜åŒ– â­ |
| FA_linear_mxfp4 | ~50% | +40% | è¾ƒä½ | æœ€å¤§ä¼˜åŒ– |

## ğŸ¯ é€‰æ‹©å»ºè®®

### æ¨èFAé‡åŒ–çš„æƒ…å†µï¼š
- âœ… æ³¨æ„åŠ›è®¡ç®—æ˜¯ç“¶é¢ˆ
- âœ… éœ€è¦ä¿æŒçº¿æ€§å±‚ç²¾åº¦
- âœ… åºåˆ—é•¿åº¦è¾ƒé•¿

### æ¨èLinearé‡åŒ–çš„æƒ…å†µï¼š
- âœ… çº¿æ€§å±‚è®¡ç®—æ˜¯ç“¶é¢ˆ
- âœ… éœ€è¦ä¿æŒæ³¨æ„åŠ›ç²¾åº¦
- âœ… æ¨¡å‹å‚æ•°é‡å¤§

### æ¨èFA+Linearé‡åŒ–çš„æƒ…å†µï¼š
- âœ… éœ€è¦æœ€å¤§å†…å­˜èŠ‚çœ
- âœ… å¯ä»¥æ¥å—ä¸€å®šç²¾åº¦æŸå¤±
- âœ… å…¨é¢ä¼˜åŒ–æ€§èƒ½

## ğŸ”„ å®Œæ•´å·¥ä½œæµç¨‹

### é‡åŒ–å®éªŒæµç¨‹
```bash
# 1. æ£€æŸ¥å½“å‰é‡åŒ–çŠ¶æ€
python3 quant_type_modifier.py --check

# 2. ä¿®æ”¹ä¸ºMXFP8é‡åŒ–
python3 quant_type_modifier.py --linear-quant mxfp8 --qk-quant mxfp8 --pv-quant mxfp8

# 3. è¿è¡Œè®­ç»ƒ
python3 train_config.py --model llama31-8b --dataset wikipedia --quantization mxfp8 --dry-run

# 4. ä¿®æ”¹ä¸ºMXFP4é‡åŒ–
python3 quant_type_modifier.py --linear-quant mxfp4 --qk-quant mxfp4 --pv-quant mxfp4

# 5. è¿è¡Œè®­ç»ƒ
python3 train_config.py --model llama31-8b --dataset wikipedia --quantization mxfp4 --dry-run

# 6. æ¢å¤åŸå§‹è®¾ç½®
python3 quant_type_modifier.py --restore
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
script/
â”œâ”€â”€ é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ train_config.py                    # ä¸»è¦è®­ç»ƒé…ç½®è„šæœ¬
â”‚   â”œâ”€â”€ flash_attention_config.py          # Flash Attentioné…ç½®è„šæœ¬
â”‚   â””â”€â”€ quant_type_modifier.py             # é‡åŒ–ç±»å‹ä¿®æ”¹å·¥å…·
â”œâ”€â”€ è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ llama31-8b/                        # LLaMA 3.1 8Bè„šæœ¬ (20ä¸ª)
â”‚   â”œâ”€â”€ llama32-1b/                        # LLaMA 3.2 1Bè„šæœ¬ (20ä¸ª)
â”‚   â””â”€â”€ deepseek2_lite/                    # DeepSeek2 Liteè„šæœ¬ (20ä¸ª)
â””â”€â”€ README.md                              # æœ¬è¯´æ˜æ–‡æ¡£
```

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. é‡åŒ–æ§åˆ¶æœºåˆ¶
- **å½“å‰çŠ¶æ€**: é‡åŒ–ç±»å‹é€šè¿‡ç¡¬ç¼–ç çš„ `custom_quant_type` å˜é‡æ§åˆ¶
- **å‚æ•°æ•ˆæœ**: `--linear-quantization` å’Œ `--attention-quantization` å‚æ•°å®é™…ä¸Šä¸å­˜åœ¨
- **å®é™…æ§åˆ¶**: éœ€è¦ä¿®æ”¹æºç ä¸­çš„ç¡¬ç¼–ç å€¼æ¥å®ç°é‡åŒ–æ§åˆ¶

### 2. ä½¿ç”¨å»ºè®®
- **å¤‡ä»½æºç **: ä¿®æ”¹å‰è¯·å¤‡ä»½åŸå§‹æºç 
- **é‡æ–°ç¼–è¯‘**: ä¿®æ”¹åå¯èƒ½éœ€è¦é‡æ–°ç¼–è¯‘
- **æµ‹è¯•éªŒè¯**: ä¿®æ”¹åè¯·æµ‹è¯•ç¡®ä¿åŠŸèƒ½æ­£å¸¸
- **ç‰ˆæœ¬æ§åˆ¶**: å»ºè®®ä½¿ç”¨Gitç®¡ç†ä¿®æ”¹

### 3. éªŒè¯ä¿®æ”¹
```bash
# ä½¿ç”¨è„šæœ¬éªŒè¯
python3 quant_type_modifier.py --check

# ä½¿ç”¨grepå‘½ä»¤éªŒè¯
grep -n "custom_quant_type" megatron/core/tensor_parallel/layers.py
grep -n "custom_quant_type" megatron/core/transformer/dot_product_attention.py
```

## ğŸ‰ æ€»ç»“

è¿™ä¸ªç³»ç»Ÿæä¾›äº†å®Œæ•´çš„é‡åŒ–è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼š

1. **3ä¸ªæ¨¡å‹**: llama31-8b, llama32-1b, deepseek2_lite
2. **2ä¸ªæ•°æ®é›†**: wikipedia, dolma
3. **10ç§é‡åŒ–æ–¹å¼**: åŒ…æ‹¬FAã€Linearã€FA+Linearçš„å®Œæ•´ç»„åˆ
4. **60ä¸ªè®­ç»ƒè„šæœ¬**: è¦†ç›–æ‰€æœ‰å¯èƒ½çš„ç»„åˆ
5. **çµæ´»æ§åˆ¶**: å¯ä»¥ç²¾ç¡®æ§åˆ¶å“ªäº›éƒ¨åˆ†è¿›è¡Œé‡åŒ–
6. **å®‰å…¨å·¥å…·**: æä¾›é‡åŒ–ç±»å‹ä¿®æ”¹å·¥å…·ï¼Œæ”¯æŒå¤‡ä»½å’Œæ¢å¤

é€šè¿‡è¿™ä¸ªç³»ç»Ÿï¼Œæ‚¨å¯ä»¥è½»æ¾è¿›è¡Œå„ç§é‡åŒ–å®éªŒï¼Œæ‰¾åˆ°æœ€é€‚åˆæ‚¨éœ€æ±‚çš„é‡åŒ–é…ç½®ï¼ğŸš€