#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†å·¥å…·å‡½æ•°
æä¾›å¸¸ç”¨çš„æ•°æ®å¤„ç†è¾…åŠ©åŠŸèƒ½
"""

import os
import json
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import time
import shutil

class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ç±»"""
    
    def __init__(self, base_dir: str = "."):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            base_dir: åŸºç¡€ç›®å½•è·¯å¾„
        """
        self.base_dir = Path(base_dir)
        self.dataset_dir = self.base_dir / "dataset"
        self.model_dir = self.base_dir / "model"
        self.tools_dir = self.base_dir / "tools"
        
    def check_environment(self) -> bool:
        """æ£€æŸ¥ç¯å¢ƒæ˜¯å¦æ»¡è¶³è¦æ±‚"""
        print("=== æ£€æŸ¥ç¯å¢ƒ ===")
        
        # æ£€æŸ¥å¿…è¦ç›®å½•
        required_dirs = [self.dataset_dir, self.model_dir, self.tools_dir]
        for dir_path in required_dirs:
            if not dir_path.exists():
                print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {dir_path}")
                return False
            print(f"âœ… ç›®å½•å­˜åœ¨: {dir_path}")
        
        # æ£€æŸ¥preprocess_data.py
        preprocess_script = self.tools_dir / "preprocess_data.py"
        if not preprocess_script.exists():
            print(f"âŒ é¢„å¤„ç†è„šæœ¬ä¸å­˜åœ¨: {preprocess_script}")
            return False
        print(f"âœ… é¢„å¤„ç†è„šæœ¬å­˜åœ¨: {preprocess_script}")
        
        return True
    
    def list_available_datasets(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ•°æ®é›†"""
        print("=== å¯ç”¨æ•°æ®é›† ===")
        
        if not self.dataset_dir.exists():
            print("æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨")
            return []
        
        datasets = []
        for item in self.dataset_dir.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°æ®æ–‡ä»¶
                data_files = list(item.glob("**/*.json*")) + list(item.glob("**/*.txt*"))
                if data_files:
                    datasets.append(item.name)
                    print(f"âœ… {item.name} ({len(data_files)} ä¸ªæ–‡ä»¶)")
                else:
                    print(f"âš ï¸  {item.name} (æ— æ•°æ®æ–‡ä»¶)")
        
        return datasets
    
    def list_available_models(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹"""
        print("=== å¯ç”¨æ¨¡å‹ ===")
        
        if not self.model_dir.exists():
            print("æ¨¡å‹ç›®å½•ä¸å­˜åœ¨")
            return []
        
        models = []
        for item in self.model_dir.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«tokenizeræ–‡ä»¶
                tokenizer_files = list(item.glob("tokenizer*")) + list(item.glob("*.json"))
                if tokenizer_files:
                    models.append(item.name)
                    print(f"âœ… {item.name}")
                else:
                    print(f"âš ï¸  {item.name} (æ— tokenizeræ–‡ä»¶)")
        
        return models
    
    def estimate_processing_time(self, input_path: str, workers: int = 16) -> str:
        """ä¼°ç®—å¤„ç†æ—¶é—´"""
        input_path = Path(input_path)
        
        if not input_path.exists():
            return "æ— æ³•ä¼°ç®—ï¼šè¾“å…¥è·¯å¾„ä¸å­˜åœ¨"
        
        # è®¡ç®—æ–‡ä»¶å¤§å°
        total_size = 0
        file_count = 0
        
        if input_path.is_file():
            total_size = input_path.stat().st_size
            file_count = 1
        else:
            for file_path in input_path.rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
                    file_count += 1
        
        # ä¼°ç®—å¤„ç†æ—¶é—´ï¼ˆåŸºäºç»éªŒå€¼ï¼‰
        # å‡è®¾æ¯GBæ•°æ®éœ€è¦çº¦10-30åˆ†é’Ÿï¼Œå–å†³äºç¡¬ä»¶é…ç½®
        size_gb = total_size / (1024**3)
        estimated_minutes = size_gb * 20  # 20åˆ†é’Ÿ/GB
        
        return f"ä¼°ç®—å¤„ç†æ—¶é—´: {estimated_minutes:.1f}åˆ†é’Ÿ (åŸºäº{size_gb:.2f}GBæ•°æ®, {file_count}ä¸ªæ–‡ä»¶)"
    
    def get_optimal_workers(self, input_path: str) -> int:
        """è·å–æœ€ä¼˜çš„å·¥ä½œè¿›ç¨‹æ•°"""
        import multiprocessing
        
        # è·å–CPUæ ¸å¿ƒæ•°
        cpu_count = multiprocessing.cpu_count()
        
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ•°é‡
        input_path = Path(input_path)
        file_count = 0
        
        if input_path.is_file():
            file_count = 1
        else:
            file_count = len(list(input_path.rglob("*")))
        
        # è®¡ç®—æœ€ä¼˜è¿›ç¨‹æ•°
        optimal_workers = min(cpu_count, max(1, file_count // 4))
        
        return optimal_workers
    
    def validate_input_data(self, input_path: str) -> Tuple[bool, str]:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        input_path = Path(input_path)
        
        if not input_path.exists():
            return False, "è¾“å…¥è·¯å¾„ä¸å­˜åœ¨"
        
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        if input_path.is_file():
            if input_path.suffix in ['.json', '.jsonl', '.txt']:
                return True, "æ–‡ä»¶æ ¼å¼æ­£ç¡®"
            else:
                return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_path.suffix}"
        else:
            # æ£€æŸ¥ç›®å½•ä¸­çš„æ–‡ä»¶
            data_files = list(input_path.rglob("*.json*")) + list(input_path.rglob("*.txt*"))
            if not data_files:
                return False, "ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶"
            
            return True, f"æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶"
    
    def create_processing_script(self, 
                                dataset_name: str,
                                input_path: str,
                                output_prefix: str,
                                tokenizer_model: str,
                                **kwargs) -> str:
        """åˆ›å»ºå¤„ç†è„šæœ¬"""
        
        script_content = f"""#!/bin/bash
# è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®å¤„ç†è„šæœ¬ - {dataset_name}
# ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}

# è®¾ç½®å‚æ•°
INPUT_PATH="{input_path}"
OUTPUT_PREFIX="{output_prefix}"
TOKENIZER_MODEL="{tokenizer_model}"
WORKERS={kwargs.get('workers', 16)}
PARTITIONS={kwargs.get('partitions', 4)}
TOKENIZER_TYPE="{kwargs.get('tokenizer_type', 'HuggingFaceTokenizer')}"
APPEND_EOD="{kwargs.get('append_eod', 'true')}"
SEQUENCE_LENGTH={kwargs.get('sequence_length', 2048)}
OVERWRITE="{kwargs.get('overwrite', 'false')}"

echo "=== {dataset_name}æ•°æ®å¤„ç† ==="
echo "è¾“å…¥è·¯å¾„: $INPUT_PATH"
echo "è¾“å‡ºå‰ç¼€: $OUTPUT_PREFIX"
echo "åˆ†è¯å™¨æ¨¡å‹: $TOKENIZER_MODEL"
echo "å·¥ä½œè¿›ç¨‹æ•°: $WORKERS"
echo "åˆ†åŒºæ•°: $PARTITIONS"

# æ„å»ºå‘½ä»¤
CMD="python tools/preprocess_data.py"
CMD="$CMD --input '$INPUT_PATH'"
CMD="$CMD --workers $WORKERS"
CMD="$CMD --partitions $PARTITIONS"
CMD="$CMD --output-prefix $OUTPUT_PREFIX"
CMD="$CMD --tokenizer-type $TOKENIZER_TYPE"
CMD="$CMD --tokenizer-model $TOKENIZER_MODEL"

if [ "$APPEND_EOD" = "true" ]; then
    CMD="$CMD --append-eod"
fi

if [ "$SEQUENCE_LENGTH" != "2048" ]; then
    CMD="$CMD --seq-length $SEQUENCE_LENGTH"
fi

if [ "$OVERWRITE" = "true" ]; then
    CMD="$CMD --overwrite"
fi

echo "æ‰§è¡Œå‘½ä»¤: $CMD"
echo "å¼€å§‹å¤„ç†æ—¶é—´: $(date)"

# æ‰§è¡Œå‘½ä»¤
eval $CMD

if [ $? -eq 0 ]; then
    echo "âœ… å¤„ç†å®Œæˆ: $(date)"
    echo "è¾“å‡ºæ–‡ä»¶:"
    ls -lh "$OUTPUT_PREFIX"*
else
    echo "âŒ å¤„ç†å¤±è´¥"
    exit 1
fi
"""
        
        # ä¿å­˜è„šæœ¬
        script_path = self.base_dir / "script" / f"process_{dataset_name}_auto.sh"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
        
        return str(script_path)
    
    def run_processing(self, 
                      input_path: str,
                      output_prefix: str,
                      tokenizer_model: str,
                      **kwargs) -> bool:
        """è¿è¡Œæ•°æ®å¤„ç†"""
        
        # éªŒè¯è¾“å…¥
        is_valid, message = self.validate_input_data(input_path)
        if not is_valid:
            print(f"âŒ è¾“å…¥éªŒè¯å¤±è´¥: {message}")
            return False
        
        print(f"âœ… è¾“å…¥éªŒè¯é€šè¿‡: {message}")
        
        # è·å–æœ€ä¼˜å‚æ•°
        optimal_workers = self.get_optimal_workers(input_path)
        if 'workers' not in kwargs:
            kwargs['workers'] = optimal_workers
            print(f"ğŸ’¡ ä½¿ç”¨æœ€ä¼˜å·¥ä½œè¿›ç¨‹æ•°: {optimal_workers}")
        
        # ä¼°ç®—å¤„ç†æ—¶é—´
        time_estimate = self.estimate_processing_time(input_path, kwargs['workers'])
        print(f"â±ï¸  {time_estimate}")
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "tools/preprocess_data.py",
            "--input", input_path,
            "--workers", str(kwargs.get('workers', 16)),
            "--partitions", str(kwargs.get('partitions', 4)),
            "--output-prefix", output_prefix,
            "--tokenizer-type", kwargs.get('tokenizer_type', 'HuggingFaceTokenizer'),
            "--tokenizer-model", tokenizer_model
        ]
        
        if kwargs.get('append_eod', True):
            cmd.append("--append-eod")
        
        if kwargs.get('sequence_length', 2048) != 2048:
            cmd.extend(["--seq-length", str(kwargs['sequence_length'])])
        
        if kwargs.get('overwrite', False):
            cmd.append("--overwrite")
        
        print(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # æ‰§è¡Œå‘½ä»¤
            result = subprocess.run(cmd, check=True, capture_output=True, text=True)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            end_time = time.time()
            duration = end_time - start_time
            
            print(f"âœ… å¤„ç†å®Œæˆ!")
            print(f"â° ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
            
            # æ˜¾ç¤ºè¾“å‡ºæ–‡ä»¶
            output_path = Path(output_prefix)
            if output_path.with_suffix('.bin').exists():
                bin_size = output_path.with_suffix('.bin').stat().st_size / (1024**2)
                idx_size = output_path.with_suffix('.idx').stat().st_size / (1024**2)
                print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶å¤§å°: .bin={bin_size:.1f}MB, .idx={idx_size:.1f}MB")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            print(f"é”™è¯¯è¾“å‡º: {e.stderr}")
            return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ•°æ®å¤„ç†å·¥å…·')
    parser.add_argument('--action', choices=['check', 'list', 'process', 'estimate'], 
                       default='check', help='æ‰§è¡Œçš„æ“ä½œ')
    parser.add_argument('--input', type=str, help='è¾“å…¥æ•°æ®è·¯å¾„')
    parser.add_argument('--output', type=str, help='è¾“å‡ºå‰ç¼€')
    parser.add_argument('--tokenizer', type=str, help='åˆ†è¯å™¨æ¨¡å‹è·¯å¾„')
    parser.add_argument('--workers', type=int, default=16, help='å·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--partitions', type=int, default=4, help='åˆ†åŒºæ•°')
    parser.add_argument('--seq-length', type=int, default=2048, help='åºåˆ—é•¿åº¦')
    parser.add_argument('--no-eod', action='store_true', help='ä¸è¿½åŠ EOD')
    parser.add_argument('--overwrite', action='store_true', help='è¦†ç›–è¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = DataProcessor()
    
    if args.action == 'check':
        processor.check_environment()
    
    elif args.action == 'list':
        processor.list_available_datasets()
        processor.list_available_models()
    
    elif args.action == 'estimate':
        if not args.input:
            print("é”™è¯¯: éœ€è¦æŒ‡å®š --input å‚æ•°")
            return
        estimate = processor.estimate_processing_time(args.input, args.workers)
        print(estimate)
    
    elif args.action == 'process':
        if not all([args.input, args.output, args.tokenizer]):
            print("é”™è¯¯: éœ€è¦æŒ‡å®š --input, --output, --tokenizer å‚æ•°")
            return
        
        kwargs = {
            'workers': args.workers,
            'partitions': args.partitions,
            'sequence_length': args.seq_length,
            'append_eod': not args.no_eod,
            'overwrite': args.overwrite
        }
        
        success = processor.run_processing(
            args.input, args.output, args.tokenizer, **kwargs
        )
        
        if not success:
            exit(1)


if __name__ == "__main__":
    main()
