#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆtensorä¿å­˜å™¨æµ‹è¯•è„šæœ¬ï¼ˆä¸ä¾èµ–PyTorchï¼‰
"""

import os
import sys
sys.path.append('/data/charles/Megatron-LM')

def test_tensor_saver_import():
    """æµ‹è¯•tensor_saveræ¨¡å—å¯¼å…¥"""
    print("=== æµ‹è¯•TensorSaveræ¨¡å—å¯¼å…¥ ===")
    
    try:
        from megatron.core.tensor_saver import TensorSaver, get_tensor_saver
        print("âœ… TensorSaveræ¨¡å—å¯¼å…¥æˆåŠŸ")
        
        # æµ‹è¯•åˆ›å»ºå®ä¾‹
        saver = TensorSaver(save_dir="./test_logs", enabled=False)
        print("âœ… TensorSaverå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å…¨å±€å®ä¾‹
        global_saver = get_tensor_saver()
        print("âœ… å…¨å±€TensorSaverå®ä¾‹è·å–æˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ TensorSaveræ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_environment_variables():
    """æµ‹è¯•ç¯å¢ƒå˜é‡è®¾ç½®"""
    print("\n=== æµ‹è¯•ç¯å¢ƒå˜é‡è®¾ç½® ===")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TENSOR_SAVE_DIR'] = './test_tensor_logs'
    os.environ['TENSOR_SAVE_ENABLED'] = 'true'
    os.environ['CUSTOM_QUANT_TYPE'] = 'hifp8'
    
    print(f"TENSOR_SAVE_DIR: {os.environ.get('TENSOR_SAVE_DIR')}")
    print(f"TENSOR_SAVE_ENABLED: {os.environ.get('TENSOR_SAVE_ENABLED')}")
    print(f"CUSTOM_QUANT_TYPE: {os.environ.get('CUSTOM_QUANT_TYPE')}")
    
    return True


def test_file_structure():
    """æµ‹è¯•æ–‡ä»¶ç»“æ„"""
    print("\n=== æµ‹è¯•æ–‡ä»¶ç»“æ„ ===")
    
    files_to_check = [
        "/data/charles/Megatron-LM/megatron/core/tensor_saver.py",
        "/data/charles/Megatron-LM/megatron/core/transformer/dot_product_attention.py",
        "/data/charles/Megatron-LM/megatron/core/tensor_parallel/layers.py",
        "/data/charles/Megatron-LM/test_tensor_saver.py",
        "/data/charles/Megatron-LM/TENSOR_SAVER_README.md"
    ]
    
    all_exist = True
    for file_path in files_to_check:
        if os.path.exists(file_path):
            print(f"âœ… {file_path}")
        else:
            print(f"âŒ {file_path}")
            all_exist = False
    
    return all_exist


def test_code_modifications():
    """æµ‹è¯•ä»£ç ä¿®æ”¹"""
    print("\n=== æµ‹è¯•ä»£ç ä¿®æ”¹ ===")
    
    # æ£€æŸ¥attentionæ–‡ä»¶ä¸­çš„ä¿®æ”¹
    attention_file = "/data/charles/Megatron-LM/megatron/core/transformer/dot_product_attention.py"
    if os.path.exists(attention_file):
        with open(attention_file, 'r') as f:
            content = f.read()
            
        checks = [
            ("tensor_saverå¯¼å…¥", "from megatron.core.tensor_saver import save_attention_tensors"),
            ("ä¿å­˜forwardè¾“å…¥tensor", "ä¿å­˜forwardè¾“å…¥tensor"),
            ("ç¯å¢ƒå˜é‡è·å–", "os.environ.get('CUSTOM_QUANT_TYPE'"),
        ]
        
        for check_name, check_text in checks:
            if check_text in content:
                print(f"âœ… {check_name}")
            else:
                print(f"âŒ {check_name}")
    
    # æ£€æŸ¥layersæ–‡ä»¶ä¸­çš„ä¿®æ”¹
    layers_file = "/data/charles/Megatron-LM/megatron/core/tensor_parallel/layers.py"
    if os.path.exists(layers_file):
        with open(layers_file, 'r') as f:
            content = f.read()
            
        checks = [
            ("tensor_saverå¯¼å…¥", "from megatron.core.tensor_saver import save_linear_tensors"),
            ("ä¿å­˜forwardè¾“å…¥tensor", "ä¿å­˜forwardè¾“å…¥tensor"),
            ("ä¿å­˜backwardè¾“å…¥tensor", "ä¿å­˜backwardè¾“å…¥tensor"),
            ("ç¯å¢ƒå˜é‡è·å–", "os.environ.get('CUSTOM_QUANT_TYPE'"),
        ]
        
        for check_name, check_text in checks:
            if check_text in content:
                print(f"âœ… {check_name}")
            else:
                print(f"âŒ {check_name}")


def test_tensor_saver_class():
    """æµ‹è¯•TensorSaverç±»çš„åŸºæœ¬åŠŸèƒ½"""
    print("\n=== æµ‹è¯•TensorSaverç±»åŸºæœ¬åŠŸèƒ½ ===")
    
    try:
        from megatron.core.tensor_saver import TensorSaver
        
        # æµ‹è¯•åˆ›å»ºå®ä¾‹
        saver = TensorSaver(save_dir="./test_logs", enabled=False)
        print("âœ… TensorSaverå®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ–‡ä»¶åç”Ÿæˆ
        filename = saver._generate_filename("attention", "forward", "hifp8", "query", 0)
        print(f"âœ… æ–‡ä»¶åç”Ÿæˆ: {filename}")
        
        # æµ‹è¯•å…ƒæ•°æ®
        metadata = {
            "test": "value",
            "layer_idx": 0
        }
        print(f"âœ… å…ƒæ•°æ®å¤„ç†: {metadata}")
        
        return True
        
    except Exception as e:
        print(f"âŒ TensorSaverç±»æµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•Tensorä¿å­˜å™¨åŠŸèƒ½...")
    
    tests = [
        test_tensor_saver_import,
        test_environment_variables,
        test_file_structure,
        test_code_modifications,
        test_tensor_saver_class,
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            if test():
                passed += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test.__name__} å¤±è´¥: {e}")
    
    print(f"\n=== æµ‹è¯•ç»“æœ ===")
    print(f"é€šè¿‡: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\nä½¿ç”¨è¯´æ˜:")
        print("1. è®¾ç½®ç¯å¢ƒå˜é‡: export CUSTOM_QUANT_TYPE='hifp8'")
        print("2. è®¾ç½®ä¿å­˜ç›®å½•: export TENSOR_SAVE_DIR='./tensor_logs'")
        print("3. å¯ç”¨ä¿å­˜åŠŸèƒ½: export TENSOR_SAVE_ENABLED='true'")
        print("4. è¿è¡Œè®­ç»ƒè„šæœ¬ï¼Œtensorå°†è‡ªåŠ¨ä¿å­˜åˆ°æŒ‡å®šç›®å½•")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ä¿®æ”¹")


if __name__ == "__main__":
    main()
