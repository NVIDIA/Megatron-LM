ENV_VARS:
  CUDA_DEVICE_MAX_CONNECTIONS: 1
  NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
  NCCL_ALGO: Ring
  CUBLAS_WORKSPACE_CONFIG: :4096:8
  N_REPEAT: 1
TEST_TYPE: frozen-start
MODE: rl
MODEL_ARGS:
  --tensor-model-parallel-size: 4
  --inference-dynamic-batching-num-cuda-graphs: 1
  --inference-dynamic-batching-unified-memory-level: 1
  --inference-dynamic-batching-buffer-size-gb: 20
  --ckpt-format: torch_dist
  --seq-length: 1024
  --inference-max-seq-length: 1024
  --load: ${CHECKPOINT_LOAD_PATH}/model/qwen3-8b-dist
  --untie-embeddings-and-output-weights: true
  --num-layers: 36
  --hidden-size: 4096
  --ffn-hidden-size: 12288
  --num-attention-heads: 32
  --kv-channels: 128
  --max-position-embeddings: 1024
  --group-query-attention: true
  --num-query-groups: 8
  --normalization: RMSNorm
  --norm-epsilon: 0.000001
  --qk-layernorm: true
  --position-embedding-type: rope
  --rotary-percent: 1.0
  --rotary-base: 1000000
  --use-rotary-position-embeddings: true
  --swiglu: true
  --disable-bias-linear: true
  --attention-dropout: 0.0
  --hidden-dropout: 0.0
  --no-masked-softmax-fusion: true
  --attention-softmax-in-fp32: true
  --tokenizer-type: HuggingFaceTokenizer
  --tokenizer-model: ${CHECKPOINT_LOAD_PATH}/model/qwen3-8b-dist/tokenizer
  --langrl-inference-server-type: inplace_megatron_chat
  --langrl-inference-server-conversation-template: ${CHECKPOINT_LOAD_PATH}/model/qwen3-8b-dist/tokenizer
  --vocab-size: 151936
  --make-vocab-size-divisible-by: 128
  --optimizer: adam
  --adam-beta1: 0.9
  --adam-beta2: 0.999
  --adam-eps: 0.00000001
  --lr: 0.000001  
  --min-lr: 0.0000001
  --lr-warmup-samples: 0
  --clip-grad: 1.0
  --weight-decay: 0.01
  --deterministic-mode: true
  --use-mcore-models: true
  --bf16: true
  --log-memory-to-tensorboard: true
  --log-num-zeros-in-grad: true
  --log-validation-ppl-to-tensorboard: true
  --log-timers-to-tensorboard: true
  --timing-log-option: minmax
  --log-throughput: true
  --no-create-attention-mask-in-dataloader: true
  --straggler-minmax-count: 16
  --tensorboard-log-interval: 1
  --empty-unused-memory-level: 2
  --seed: 42
  --calculate-per-token-loss: true
  --rl-use-sequence-packing: true
  --rl-sequence-packing-algo: fifo
  --rl-offload-optimizer-during-inference: true
  --timing-log-level: 1
  --log-interval: 1
  --log-progress: true
  --cuda-graph-impl: local
  --micro-batch-size: 1
  --global-batch-size: 4
  --grpo-group-size: 2
  --grpo-prompts-per-step: 2
  --grpo-iterations: 1
  --grpo-clamp-eps-lower: 0.2
  --grpo-clamp-eps-upper: 0.2
  --grpo-kl-beta: 0.0
  --grpo-entropy-term-weight: 0.0
  --langrl-env-config: tests/functional_tests/test_cases/gpt/gpt_grpo_tp4_pp1_dp2_8b_throughput_github/env_config.yaml
  --rl-partial-rollouts: true
  --use-checkpoint-args: true
  --dist-ckpt-strictness: log_unexpected
  --perform-rl-step: true
  --train-samples: 48828125
  --exit-interval: 20
  --tensorboard-dir: ${TENSORBOARD_PATH}
  --save-interval: 1000000
  --eval-interval: 1000000
  --finetune: true
  --inference-logging-step-interval: 1
METRICS:
  - "mem-allocated-bytes"
  - "mem-max-allocated-bytes"
  - "iteration-time"
THROUGHPUT_TEST_PARAMS:
  --start_step: 10
