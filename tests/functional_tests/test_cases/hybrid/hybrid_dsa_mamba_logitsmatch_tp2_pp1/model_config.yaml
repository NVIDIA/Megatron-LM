ENV_VARS:
  CUDA_DEVICE_MAX_CONNECTIONS: 1
  NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
  NCCL_ALGO: Ring
  CUBLAS_WORKSPACE_CONFIG: ":4096:8"
TEST_TYPE: frozen-start
MODE: inference
MODEL_ARGS:
  --load: ${CHECKPOINT_LOAD_PATH}/model/deepseek_v3_proxy/dcp/checkpoint
  --tokenizer-type: TikTokenizer
  --tiktoken-pattern: v2
  --transformer-impl: transformer_engine
  --tensor-model-parallel-size: 2
  --pipeline-model-parallel-size: 1
  --use-mcore-models: true
  --is-hybrid-model: true
  --model-provider: mamba
  --disable-bias-linear: true
  --position-embedding-type: rope
  --multi-latent-attention: true
  --q-lora-rank: 64
  --kv-lora-rank: 64
  --qk-head-dim: 64
  --qk-pos-emb-head-dim: 32
  --v-head-dim: 64
  --dsa-indexer-n-heads: 8
  --dsa-indexer-head-dim: 64
  --dsa-indexer-topk: 32
  --num-layers: 8
  --hidden-size: 256
  --num-attention-heads: 16
  --hybrid-override-pattern: "S-S-S-S-"
  --spec: megatron.core.models.mamba.mamba_layer_specs mamba_stack_spec
  --normalization: RMSNorm
  --swiglu: true
  --bf16: true
  --attention-backend: flash
  --deterministic-mode: true
  --temperature: 1.0
  --top_k: 1
  --return-log-probs: true
  --num-tokens-to-generate: 30
  --inference-max-seq-length: 256
  --output-path: ${INFERENCE_OUTPUT_PATH}
  --prompts: "The quick brown fox jumps over the lazy dog."
  --incoming-requests-per-sec: -1
METRICS:
  - "generated_tokens"
  - "logprobs"
