ENV_VARS:
  CUDA_DEVICE_MAX_CONNECTIONS: 32
  NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
  NCCL_ALGO: Ring
  CUBLAS_WORKSPACE_CONFIG: :4096:8
  PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
  NCCL_NVLS_ENABLE: 0
  NVTE_FWD_LAYERNORM_SM_MARGIN: 8
  NVTE_BWD_LAYERNORM_SM_MARGIN: 8
  TORCH_NCCL_AVOID_RECORD_STREAMS: 1
  N_REPEATS: 5
MODEL_ARGS:
  --num-layers: 8
  --hidden-size: 2048
  --num-attention-heads: 16
  --log-params-norm: true
  --log-num-zeros-in-grad: true
  --log-validation-ppl-to-tensorboard: true
  --log-timers-to-tensorboard: true
  --tensorboard-dir: ${TENSORBOARD_PATH}
  --micro-batch-size: 1
  --global-batch-size: 32
  --seq-length: 2048
  --max-position-embeddings: 2048
  --train-iters: 50
  --timing-log-level: 2
  --lr-decay-iters: 320000
  --save: ${CHECKPOINT_SAVE_PATH}
  --load: ${CHECKPOINT_LOAD_PATH}
  --data-path: ${DATA_PATH}/my-gpt3_00_text_document
  --vocab-file: ${DATA_PATH}/bpe/vocab.json
  --merge-file: ${DATA_PATH}/bpe/merges.txt
  --split: 949,50,1
  --distributed-backend: nccl
  --lr: 0.00015
  --lr-decay-style: cosine
  --min-lr: 1.0e-5
  --weight-decay: 1e-2
  --clip-grad: 1.0
  --lr-warmup-fraction: .01
  --log-interval: 1
  --save-interval: 10000
  --eval-interval: 1000
  --eval-iters: 10
  --transformer-impl: transformer_engine
  --tensor-model-parallel-size: 1
  --pipeline-model-parallel-size: 4
  --num-layers-per-virtual-pipeline-stage: 1
  --expert-model-parallel-size: 2
  --expert-tensor-parallel-size: 1
  --disable-bias-linear: true
  --sequence-parallel: true
  --num-experts: 8
  --moe-router-load-balancing-type: aux_loss
  --moe-router-topk: 2
  --moe-aux-loss-coeff: 1e-2
  --deterministic-mode: true
  --moe-grouped-gemm: true
  --moe-ffn-hidden-size: 2048
  --attention-softmax-in-fp32: true
  --use-mcore-models: true
  --ckpt-format: torch_dist
  --dist-ckpt-strictness: log_all  # backward compatibility for TE changes
  --data-cache-path: ${DATA_CACHE_PATH}
  --bf16: true
  --attention-backend: unfused
  --log-memory-to-tensorboard: true
  --combined-1f1b: true
  --combined-1f1b-recipe: ep_a2a
  --moe-token-dispatcher-type: alltoall
TEST_TYPE: regular
