ENV_VARS:
  CUDA_DEVICE_MAX_CONNECTIONS: 1
  NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
  PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
  NCCL_NVLS_ENABLE: 0
  PYTHONWARNINGS: ignore
  NCCL_DEBUG: VERSION

MODEL_ARGS:
  # Transformer Engine args
  --transformer-impl: transformer_engine
  # Distributed args
  --distributed-timeout-minutes: 60
  --tensor-model-parallel-size: 1
  --pipeline-model-parallel-size: 1
  --expert-model-parallel-size: 8
  --context-parallel-size: 1
  --expert-tensor-parallel-size: 1
  # NOTE: Muon optimizer does not support distributed optimizer
  # --use-distributed-optimizer: true
  --use-mcore-models: true
  --sequence-parallel: true
  --micro-batch-size: 4
  # MoE training related args
  --moe-grouped-gemm: true
  --moe-token-dispatcher-type: allgather
  --save-interval: 25
  # Muon optimizer args
  --optimizer: muon
  --muon-momentum: 0.9
  --muon-extra-scale-factor: 0.2
  --muon-scale-mode: spectral
  --use-checkpoint-opt_param-scheduler: true
  # Add mixed precision args
  --bf16: true
  --exit-interval: 50
  # kernel fusion related args
  --no-rope-fusion: true
  --cross-entropy-loss-fusion: true
  --cross-entropy-fusion-impl: native
  # MISC
  --manual-gc: true
  --manual-gc-interval: 100
TEST_TYPE: regular

