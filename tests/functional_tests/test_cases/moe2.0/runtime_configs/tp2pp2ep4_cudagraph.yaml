ENV_VARS:
  CUDA_DEVICE_MAX_CONNECTIONS: 1
  NVTE_ALLOW_NONDETERMINISTIC_ALGO: 0
  # NOTE: expandable_segments must be commented out for cuda graph to work
  # PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
  NCCL_NVLS_ENABLE: 0
  PYTHONWARNINGS: ignore
  NCCL_DEBUG: VERSION
  NCCL_ALGO: Ring
  CUBLAS_WORKSPACE_CONFIG: ":4096:8"

MODEL_ARGS:
  # Transformer Engine args
  --transformer-impl: transformer_engine
  # Distributed args
  --distributed-timeout-minutes: 60
  --tensor-model-parallel-size: 2
  --pipeline-model-parallel-size: 2
  --num-virtual-stages-per-pipeline-rank: 4
  --expert-model-parallel-size: 4
  --context-parallel-size: 1
  --expert-tensor-parallel-size: 1
  --use-distributed-optimizer: true
  --overlap-grad-reduce: true
  --overlap-param-gather: true
  --use-mcore-models: true
  --sequence-parallel: true
  --micro-batch-size: 4
  # MoE training related args
  --moe-grouped-gemm: true
  --moe-token-dispatcher-type: alltoall
  # CUDA Graph args
  --cuda-graph-impl: transformer_engine
  --cuda-graph-scope: "[attn mlp moe_router moe_preprocess]"
  --cuda-graph-warmup-steps: 1
  --te-rng-tracker: true
  # Add mixed precision args
  --bf16: true
  --fp8-format: hybrid
  --fp8-recipe: blockwise
  --first-last-layers-bf16: true
  # kernel fusion related args
  --moe-permute-fusion: true
  --moe-router-fusion: true
  # Selective recompute for moe_act
  --recompute-granularity: selective
  --recompute-modules: "[moe_act]"
  # MISC
  --manual-gc: true
  --manual-gc-interval: 100
TEST_TYPE: resume-ckpt

