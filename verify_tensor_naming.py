#!/usr/bin/env python3
"""
éªŒè¯æ”¹è¿›çš„tensorå‘½ååŠŸèƒ½
ç¡®ä¿æ‰€æœ‰ä¿®æ”¹éƒ½æ­£ç¡®å·¥ä½œ
"""

import os
import sys
import torch

def verify_tensor_saver():
    """éªŒè¯tensor_saveræ¨¡å—"""
    print("=== éªŒè¯TensorSaveræ¨¡å— ===")
    
    try:
        from megatron.core.tensor_saver import TensorSaver, save_tensor, save_attention_tensors, save_linear_tensors
        print("âœ… æˆåŠŸå¯¼å…¥æ‰€æœ‰tensor_saverå‡½æ•°")
        
        # æµ‹è¯•TensorSaverç±»
        saver = TensorSaver(save_dir="./test_logs", enabled=True)
        print("âœ… æˆåŠŸåˆ›å»ºTensorSaverå®ä¾‹")
        
        # æµ‹è¯•æ–‡ä»¶åç”Ÿæˆ
        filename = saver._generate_filename(
            layer_type="attention",
            operation="forward",
            quant_type="hifp8",
            tensor_name="query",
            layer_idx=0,
            phase="pre",
            component="FA"
        )
        print(f"âœ… æ–‡ä»¶åç”Ÿæˆæµ‹è¯•: {filename}")
        
        # éªŒè¯æ–‡ä»¶åæ ¼å¼
        expected_parts = ["attention", "L0", "forward", "pre", "FA", "hifp8", "query"]
        for part in expected_parts:
            if part in filename:
                print(f"  âœ… åŒ…å« {part}")
            else:
                print(f"  âŒ ç¼ºå°‘ {part}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ TensorSaveréªŒè¯å¤±è´¥: {e}")
        return False

def verify_attention_layer():
    """éªŒè¯attentionå±‚ä¿®æ”¹"""
    print("\n=== éªŒè¯Attentionå±‚ä¿®æ”¹ ===")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        attention_file = "/data/charles/Megatron-LM/megatron/core/transformer/dot_product_attention.py"
        if not os.path.exists(attention_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {attention_file}")
            return False
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(attention_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥å…³é”®ä¿®æ”¹
        checks = [
            ('phase="pre"', 'preé˜¶æ®µå‚æ•°'),
            ('component="FA"', 'FAç»„ä»¶å‚æ•°'),
            ('phase="post"', 'posté˜¶æ®µå‚æ•°'),
            ('save_attention_tensors', 'attention tensorä¿å­˜å‡½æ•°'),
            ('save_tensor', 'tensorä¿å­˜å‡½æ•°')
        ]
        
        for check, description in checks:
            if check in content:
                print(f"  âœ… {description}: æ‰¾åˆ° {check}")
            else:
                print(f"  âŒ {description}: æœªæ‰¾åˆ° {check}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ Attentionå±‚éªŒè¯å¤±è´¥: {e}")
        return False

def verify_linear_layer():
    """éªŒè¯linearå±‚ä¿®æ”¹"""
    print("\n=== éªŒè¯Linearå±‚ä¿®æ”¹ ===")
    
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        linear_file = "/data/charles/Megatron-LM/megatron/core/tensor_parallel/layers.py"
        if not os.path.exists(linear_file):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {linear_file}")
            return False
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(linear_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ£€æŸ¥å…³é”®ä¿®æ”¹
        checks = [
            ('phase="pre"', 'preé˜¶æ®µå‚æ•°'),
            ('component="linear"', 'linearç»„ä»¶å‚æ•°'),
            ('phase="post"', 'posté˜¶æ®µå‚æ•°'),
            ('save_linear_tensors', 'linear tensorä¿å­˜å‡½æ•°'),
            ('save_tensor', 'tensorä¿å­˜å‡½æ•°')
        ]
        
        for check, description in checks:
            if check in content:
                print(f"  âœ… {description}: æ‰¾åˆ° {check}")
            else:
                print(f"  âŒ {description}: æœªæ‰¾åˆ° {check}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ Linearå±‚éªŒè¯å¤±è´¥: {e}")
        return False

def verify_function_signatures():
    """éªŒè¯å‡½æ•°ç­¾å"""
    print("\n=== éªŒè¯å‡½æ•°ç­¾å ===")
    
    try:
        from megatron.core.tensor_saver import save_tensor, save_attention_tensors, save_linear_tensors
        import inspect
        
        # æ£€æŸ¥save_tensorå‡½æ•°ç­¾å
        sig = inspect.signature(save_tensor)
        params = list(sig.parameters.keys())
        expected_params = ['tensor', 'layer_type', 'operation', 'quant_type', 'tensor_name', 'layer_idx', 'phase', 'component', 'metadata']
        
        for param in expected_params:
            if param in params:
                print(f"  âœ… save_tensoråŒ…å«å‚æ•°: {param}")
            else:
                print(f"  âŒ save_tensorç¼ºå°‘å‚æ•°: {param}")
                return False
        
        # æ£€æŸ¥save_attention_tensorså‡½æ•°ç­¾å
        sig = inspect.signature(save_attention_tensors)
        params = list(sig.parameters.keys())
        expected_params = ['query', 'key', 'value', 'quant_type', 'operation', 'layer_idx', 'phase', 'component', 'metadata']
        
        for param in expected_params:
            if param in params:
                print(f"  âœ… save_attention_tensorsåŒ…å«å‚æ•°: {param}")
            else:
                print(f"  âŒ save_attention_tensorsç¼ºå°‘å‚æ•°: {param}")
                return False
        
        # æ£€æŸ¥save_linear_tensorså‡½æ•°ç­¾å
        sig = inspect.signature(save_linear_tensors)
        params = list(sig.parameters.keys())
        expected_params = ['input_tensor', 'weight', 'quant_type', 'operation', 'layer_idx', 'phase', 'component', 'metadata']
        
        for param in expected_params:
            if param in params:
                print(f"  âœ… save_linear_tensorsåŒ…å«å‚æ•°: {param}")
            else:
                print(f"  âŒ save_linear_tensorsç¼ºå°‘å‚æ•°: {param}")
                return False
        
        return True
        
    except Exception as e:
        print(f"âŒ å‡½æ•°ç­¾åéªŒè¯å¤±è´¥: {e}")
        return False

def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    print("å¼€å§‹éªŒè¯æ”¹è¿›çš„tensorå‘½ååŠŸèƒ½...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUSTOM_QUANT_TYPE'] = 'hifp8'
    os.environ['TENSOR_SAVE_DIR'] = './test_logs'
    os.environ['TENSOR_SAVE_ENABLED'] = 'true'
    
    # è¿è¡ŒéªŒè¯
    results = []
    results.append(verify_tensor_saver())
    results.append(verify_attention_layer())
    results.append(verify_linear_layer())
    results.append(verify_function_signatures())
    
    # æ€»ç»“ç»“æœ
    print("\n=== éªŒè¯ç»“æœæ€»ç»“ ===")
    passed = sum(results)
    total = len(results)
    
    if passed == total:
        print(f"ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡! ({passed}/{total})")
        print("âœ… æ”¹è¿›çš„tensorå‘½ååŠŸèƒ½å·²æ­£ç¡®å®ç°")
        return True
    else:
        print(f"âŒ éªŒè¯å¤±è´¥! ({passed}/{total})")
        print("è¯·æ£€æŸ¥ä¸Šè¿°å¤±è´¥çš„éªŒè¯é¡¹")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
